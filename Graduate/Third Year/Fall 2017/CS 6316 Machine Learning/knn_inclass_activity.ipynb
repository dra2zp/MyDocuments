{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CS 6316 : Machine Learning\n",
    "## In-class activity: K-Nearest Neighbor Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this activity, you will further explore KNN algorithm through python code.\n",
    "\n",
    "The data set we’ll be using is the Iris Flower Dataset (IFD) which was first introduced in 1936 by the famous statistician Ronald Fisher and consists of 50 observations from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals. Our goal is to train the KNN algorithm to be able to distinguish the species from one another given the measurements of the 4 features.\n",
    "\n",
    "Lets start by running an example code using scikit-learn to train a KNN classifier and evaluate its performance on the data set.\n",
    "There is 4 step modeling pattern:\n",
    "\n",
    "1. Import the learning algorithm\n",
    "2. Instantiate the model\n",
    "3. Learn the model\n",
    "4. Predict the response\n",
    "\n",
    "\n",
    "The following steps will print out accuracy score for K=3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) loading libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# define column names\n",
    "names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\n",
    "\n",
    "# loading training data\n",
    "df = pd.read_csv('iris.data.txt', header=None, names=names)\n",
    "df.head()\n",
    "\n",
    "# making our predictions \n",
    "predictions = []\n",
    "\n",
    "# create design matrix X and target vector y\n",
    "X = np.array(df.ix[:, 0:4])     # end index is exclusive\n",
    "y = np.array(df['class'])   # another way of indexing a pandas df\n",
    "\n",
    "# split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try traininging and comparing performances for different k values (from 1 to 50)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating odd list of K for KNN\n",
    "myList = list(range(1,50))\n",
    "\n",
    "# subsetting just the odd ones\n",
    "neighbors = list(filter(lambda x: x % 2 != 0, myList))\n",
    "\n",
    "# empty list that will hold cross validation scores\n",
    "cv_scores = []\n",
    "\n",
    "# perform 10-fold cross validation we are already familiar with\n",
    "for k in neighbors:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X_train, y_train, cv=10, scoring='accuracy')\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "# changing to misclassification error\n",
    "MSE = [1 - x for x in cv_scores]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's try to discover what is the best value of k\n",
    "# Run the following cell to show the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"k          Score          MSE\")\n",
    "for i in range(len(neighbors)):\n",
    "    print ('%d          %.5f          %.5f' % (neighbors[i], cv_scores[i], MSE[i]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the optimal value of K?\n",
    "\n",
    "Write your answer here: ____________\n",
    "(Double-click cell to edit)\n",
    "\n",
    "\n",
    "Lets make a plot to visually compare performances at each k value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determining best k\n",
    "optimal_k = neighbors[MSE.index(min(MSE))]\n",
    "print(\"The optimal number of neighbors is %d\" % optimal_k)\n",
    "\n",
    "# plot misclassification error vs k\n",
    "plt.plot(neighbors, MSE)\n",
    "plt.xlabel('Number of Neighbors K')\n",
    "plt.ylabel('Misclassification Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm that the optimal value of K (see above)\n",
    "\n",
    "Now, we are going to implement KNN prediction from scratch.\n",
    "\n",
    "\n",
    "Note that KNN is a instance-based learning, which means that our algorithm doesn’t explicitly learn a model. Instead, it chooses to memorize the training instances which are subsequently used as “knowledge” for the prediction phase. Concretely, this means that only when a query to our database is made (i.e. when we ask it to predict a label given an input), will the algorithm use the training instances to spit out an answer.\n",
    "\n",
    "In our implementation, the training block reduces to just memorizing the training data.\n",
    "This mean it just returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(X_train, y_train):\n",
    "    # do nothing \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The minimal training phase of KNN comes both at a memory cost, since we must store a potentially huge data set, as well as a computational cost during test time since classifying a given observation requires a run down of the whole data set. Practically speaking, this is undesirable since we usually want fast responses.\n",
    "\n",
    "\n",
    "Now you are going to fill in some missing lines in predict phase. Here is general workflow:\n",
    "1. Compute the euclidean distance between the “new” observation and all the data points in the training set.\n",
    "2. Select the K nearest ones and perform a majority vote.\n",
    "3. Assigns the corresponding label to the observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def predict(X_train, y_train, x_test, k):\n",
    "    # create list for distances and targets\n",
    "    distances = []\n",
    "    targets = []\n",
    "\n",
    "    for i in range(len(X_train)):\n",
    "        # first we compute the Euclidean distance\n",
    "        # (use x_test and X_train[i, :]. Also, where appropriate, you can use np.sqrt, np.square, and np.sum...)\n",
    "        distance = __________\n",
    "        # add it to list of distances\n",
    "        distances.append([distance, i])\n",
    "\n",
    "    # sort the list\n",
    "    distances = sorted(distances)\n",
    "\n",
    "    # make a list of the k neighbors' targets\n",
    "    for i in range(k):\n",
    "        # (Hint: index receives particular value in distances[something][something])\n",
    "        index = __________\n",
    "        # (Hint: use y_train and index below)\n",
    "        targets.append(__________)\n",
    "\n",
    "    # return most common target\n",
    "    return Counter(targets).most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets test our implementation, using the predict() method.\n",
    "\n",
    "*** NOTE: Fill in \"optimalK\" below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def kNearestNeighbor(X_train, y_train, X_test, predictions, k):\n",
    "    # train on the input data\n",
    "    train(X_train, y_train)\n",
    "\n",
    "    # loop over all observations\n",
    "    for i in range(len(X_test)):\n",
    "        predictions.append(predict(X_train, y_train, X_test[i, :], k))\n",
    "        \n",
    "# making our predictions \n",
    "# Using the optimal value of K discovered above\n",
    "predictions = []\n",
    "try:\n",
    "    optimalK = _______ # Add your answer here (and delete line!)\n",
    "    kNearestNeighbor(X_train, y_train, X_test, predictions, optimalK)\n",
    "    predictions = np.asarray(predictions)\n",
    "\n",
    "    # evaluating accuracy\n",
    "    accuracy = accuracy_score(y_test, predictions) * 100\n",
    "    print('\\nThe accuracy of OUR classifier is %d%%' % accuracy)\n",
    "\n",
    "except ValueError:\n",
    "    print('Can\\'t have more neighbors than training samples!!') # Need to be careful about value of k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Please turn in your work in pdf format by clicking \"File/download as/pdf\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
